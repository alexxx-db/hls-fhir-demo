{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "520e6679-deec-42c1-89d1-abd5374a08be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "*Note:  This notebooks should be executed against a Serverelss SQL Warehouse.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78d394e-808b-459d-a1cc-5f7abbf42fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***\n",
    "# Streaming Bronze Tables Ingestion Using Autoloader\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32cbb80c-f418-41d3-888c-fc6aea09837a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up the Declared Variables and the Schema to Use\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd755e2-27f7-4606-a19a-2b1e318e62f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set the Schema to Use as a Declared Variable"
    }
   },
   "outputs": [],
   "source": [
    "DECLARE OR REPLACE VARIABLE schema_use STRING DEFAULT REPLACE(SPLIT(current_user(), '@')[0], '.', '_');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fabb7a-86d7-441e-8322-2a4bb979f0cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use Catalog and Schema Statement"
    }
   },
   "outputs": [],
   "source": [
    "USE IDENTIFIER(\"fhir_workshop.\" || schema_use);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11762452-0650-454e-bca8-e37bd23bdd03",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check the Current Catalog and Schema is Set"
    }
   },
   "outputs": [],
   "source": [
    "SELECT current_catalog(), current_schema();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4d256f0-4a7e-4a15-8040-ea74b7c97bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reviewing Data in Volumes\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c451796d-9d6b-43e5-a7e0-32e8a1d61a49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show Volumes Available in the Catalog.Schema"
    }
   },
   "outputs": [],
   "source": [
    "SHOW VOLUMES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596f9abb-000e-40d8-8001-838757fe80f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can print details about the files in volume using SQL with the `LIST` statement.  \n",
    "\n",
    "`LIST <volume_path>;`\n",
    "\n",
    "Note that we've added a `LIMIT` at the end of the `LIST` statement to only print the first 100 records.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2104132-ea0e-4e83-87e9-271d8a4c498f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dynamically Build the LIST statement"
    }
   },
   "outputs": [],
   "source": [
    "DECLARE OR REPLACE VARIABLE list_stmnt STRING;\n",
    "\n",
    "SET VARIABLE list_stmnt = \"LIST '/Volumes/fhir_workshop/\" || schema_use || \"/landing/' LIMIT 100;\";\n",
    "\n",
    "SELECT list_stmnt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39b7b00-6b3c-4092-89f4-f48c2e56d786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we've dynamically built a SQL statement by declaring and setting a string variable that contains our SQL statement we wish to execute simply by concatentating other SQL variables together.  We use EXECUTE IMMEDIATE to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b30497d-1067-4098-8ebd-8f195c908ca8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute the LIST Statement"
    }
   },
   "outputs": [],
   "source": [
    "EXECUTE IMMEDIATE list_stmnt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d312a319-d8e7-4225-b2ed-dd5241ed40bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using DB Serverless SQL to Create or Refresh Streaming Tables\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0e3371-dc4f-4e88-a1a0-f6544d4514f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Executing `CREATE OR REFRESH STREAMING TABLE` statements against a Serverless SQL warehouse automatically creates a Lakeflow (DLT) pipeline beheind the scenes using Serverless compute.  This is the easiest way to create a streaming pipeline.  Databricks automatically takes care of capturing the state, checkpoints, cluster size, and pipeline creation to execute the streaming table.  Users only have to worry about writing SQL.  \n",
    "\n",
    "All streaming tables are \"batch\" unless the `CONTINOUS` statement is added to the CRST statement:  `CREATE OR REFRESH CONTINOUS STREAMING TABLE`.  If a streaming table has already been defined then the `REFRESH` statement will process any data that is ready to be processed.  Once all data that is available has been processed, the stream is automatically stopped until the next `REFRESH` is trigged.  \n",
    "\n",
    "Normally we would never use the `DROP TABLE` statement.  For normal managed tables the `CREATE OR REPLACE` statement does the same thing as a truncate and reload while preserving the delta log, allowing for a back out strategy should the load need to be reversed.  This allows for better **DataOps**.  However since streaming tables are essentially Lakeflow (DLT) pipelines, a `FULL REFRESH` is essentially the same as a `DROP TABLE` combined with a `REFRESH`. \n",
    "\n",
    "Thefore, with streaming tables (or materialized views), we'll use the `FULL REFRESH` statement when we wish to completely reload the data over, and `DROP TABLE` followed by a `CREATE OR REFRESH STREAMING TABLE` statement when we need to be explict with our schema evolution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee320fef-a187-47f1-8f8f-6e399f69c9fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Refresh of the FHIR Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS fhir_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe3ad10-0388-4027-aa33-849709880298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We're going to create two bronze tables.  The first bronze will use autoloader and therefore will need to be dynamic due to the differences in the volume path's here.  Recall that a Volume path always has a root of `/Volumes/<catalog>/<schema>/<volume_name>/`.  This means that pretty much any code that will go through a CI/CD process will need dynamic SQL to incorporate changing volume paths based on catalog or schema names based on environments.  \n",
    "\n",
    "For this first bronze table, we'll use autoloaders SQL function `read_files`.  There are many different file formats that can be automatically loaded to a bronze table that will infer its schema, including JSON.  However this requires each JSON file to have the exact same schema, and due to the nature of FHIR data, no two JSONs are gurranteed to have the same schema.  This implies that normal methods for parsing JSON files are not possible with FHIR. \n",
    "\n",
    "Thankfully, we now have the VARIANT data type, which is able to parse every JSON file completely independently, including inside a stream.  To use the VARIANT data type we must either use a Databricks SQL Warehouse or Databricks Runtime 15.3LTS+.  VARIANT is a public preview data type in the open source Delta Lake file format which is the default managed table file format in Databricks.  In order to use VARIANT we'll need to turn on the feature in our Delta Tables, and set the Lakeflow Pipeline channel to preview.  VARIANT will be generally available fairly soon.  While in public preview its fully supported by Databricks and may be used for production workflows.  \n",
    "\n",
    "To transform a JSON string to VARIANT we use the `parse_json` Spark SQL function.  One thing to note about parse_json is that it must be given well formed JSON strings.  If the JSON is malformed then `parse_json` will return an error.  To simply return a NULL instead of an error we may use the `try_parse_json` function instead.  \n",
    "\n",
    "In order to have a record of everything that we've recieved from our FHIR source, we'll first ingest the bundles as text and then use try_parse_json in a following bronze table. \n",
    "\n",
    "Note that Synthea produces some malformed FHIR JSONs on purpose to simulate what you'd expect in a real world setting where you FHIR is coming form multiple sources originally.  This is where brokers like Redox come in-- Redox ensures that each transaction your to recieve is always well formed and follows the same general best practices as every Redox bundle produced.  This ensures better uniformity downstream.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b7e8b43-cfff-46fc-929f-f5d8637d8f8d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dynamically Create the FHIR Bronze Streaming Table Statement"
    }
   },
   "outputs": [],
   "source": [
    "DECLARE OR REPLACE VARIABLE create_streaming_bronze_stmnt STRING;\n",
    "\n",
    "SET VARIABLE create_streaming_bronze_stmnt = \"\n",
    "CREATE OR REFRESH STREAMING TABLE fhir_bronze (\n",
    "  file_metadata STRUCT<\n",
    "    file_path: STRING,\n",
    "    file_name: STRING,\n",
    "    file_size: BIGINT,\n",
    "    file_block_start: BIGINT,\n",
    "    file_block_length: BIGINT,\n",
    "    file_modification_time: TIMESTAMP\n",
    "  > NOT NULL COMMENT 'Original meta date of the file ingested from the volume.'\n",
    "  ,ingest_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP() COMMENT 'The date timestamp the file was ingested.'\n",
    "  ,bundle_uuid STRING NOT NULL COMMENT 'Unique identifier for the FHIR bundle.'\n",
    "  ,value STRING COMMENT 'Original JSON record ingested from the volume as a full text string value.'\n",
    ")\n",
    "COMMENT 'Ingest FHIR JSON records as Full Text STRING'\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true',\n",
    "  'delta.enableDeletionVectors' = 'true',\n",
    "  'delta.enableRowTracking' = 'true',\n",
    "  'quality' = 'bronze'\n",
    ")\n",
    "AS SELECT\n",
    "  _metadata as file_metadata\n",
    "  ,uuid() as bundle_uuid\n",
    "  ,* \n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/fhir_workshop/\" || schema_use || \"/landing/'\n",
    "  ,format => 'text'\n",
    "  ,wholeText => true\n",
    ")\n",
    "\";\n",
    "\n",
    "SELECT create_streaming_bronze_stmnt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddfa7378-24ac-47a8-9fd0-6be638d5a0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the above SQL note the use of the DDL to set the column types, defaults and comments.  We also apply a table comment, and table properties.  The table properties here are critical for the efficient processing of our streaming data to ensure we always apply incremental updates, including for later SQL that will require `PIVOT`s or materialized views.  Setting your table properties correctly vastly improves performance and lowers compute costs.  \n",
    "\n",
    "- **Enabling Change Data Feed** allows the stream or a data engineer to be able to read exactly from a particular microbatch.  \n",
    "- **Deletion vectors**, now enabled by default on new managed Delta tables performs a soft delete using meta data on initial processing, and then later a background process physcially deletes the records from the delta tables, vastly increasing performance during active loads.  \n",
    "- **Row Tracking** adds additional metadata such that streams or materialized views can be sure which data has been recently updated, inserted or deleted, resulting in incremental loads or streams inside of full refreshes.  This is required to use `PIVOT` in streaming SQL.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517aa813-001f-4ea0-aff6-4923106742f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Excecute the FHIR Bronze Statement"
    }
   },
   "outputs": [],
   "source": [
    "EXECUTE IMMEDIATE create_streaming_bronze_stmnt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3741825c-69e6-4014-878e-873983325c9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query the First 10 Rows of the FHIR Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "  file_metadata\n",
    "  ,ingest_time\n",
    "  ,bundle_uuid\n",
    "  ,substr(value FROM 0 FOR 10000) as value -- note that these bundles are so large that attempting to print even one as a full string yields results that are too large for the notebook to display! \n",
    "  -- ,value\n",
    "FROM \n",
    "  fhir_bronze \n",
    "limit 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b47530-af03-48b4-987d-da3094c910c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Record Count"
    }
   },
   "outputs": [],
   "source": [
    "select count(*) as rcrd_cnt from fhir_bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb20b2fd-6b55-4bc2-b11d-a583dac1cfd5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full Refresh of FHIR Bronze with Variant"
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS fhir_bronze_variant;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412521b1-1f1f-4533-9739-e1856f22e6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we'll execute our second streaming bronze table and turn the string value column into a fhir VARIANT column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ba4e4c-d535-4b1e-9402-0e6705a34668",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create or Refresh FHIR Bronze Variant Streaming Table"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE fhir_bronze_variant (\n",
    "  bundle_uuid STRING NOT NULL COMMENT 'Unique identifier for the FHIR bundle.'\n",
    "  ,ingest_time TIMESTAMP NOT NULL COMMENT 'The date timestamp the file was ingested.'\n",
    "  ,file_metadata STRUCT<\n",
    "    file_path: STRING,\n",
    "    file_name: STRING,\n",
    "    file_size: BIGINT,\n",
    "    file_block_start: BIGINT,\n",
    "    file_block_length: BIGINT,\n",
    "    file_modification_time: TIMESTAMP\n",
    "  > NOT NULL COMMENT 'Original meta date of the file ingested from the volume.'\n",
    "  ,fhir VARIANT COMMENT 'Original JSON record fully parsed as a variant data type.'\n",
    ")\n",
    "COMMENT 'Evaluate FHIR JSON records as VARIANT'\n",
    "TBLPROPERTIES (\n",
    "  'delta.enableChangeDataFeed' = 'true'\n",
    "  ,'delta.enableDeletionVectors' = 'true' \n",
    "  ,'delta.enableRowTracking' = 'true'\n",
    "  ,'quality' = 'bronze'\n",
    "  ,'pipelines.channel' = 'PREVIEW'\n",
    "  ,'delta.feature.variantType-preview' = 'supported'\n",
    ")\n",
    "AS SELECT\n",
    "  bundle_uuid\n",
    "  ,ingest_time\n",
    "  ,file_metadata\n",
    "  ,try_parse_json(value) as fhir \n",
    "FROM STREAM fhir_bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62b9fc7-44b1-42e8-a500-09c9e2d9a45f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query the First Few Records of the Bronze Variant Table"
    }
   },
   "outputs": [],
   "source": [
    "select \n",
    "  bundle_uuid\n",
    "  ,ingest_time\n",
    "  ,file_metadata\n",
    "  ,fhir\n",
    "from \n",
    "  fhir_bronze_variant \n",
    "where fhir is not null \n",
    "limit 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "290fe718-7f81-4513-90dc-1b6bda20c077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Swith to the SQL Editior Queries to Review the Variant parsed JSON if results are too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9711801-1acd-4e0c-86ef-1608727a2c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***\n",
    "### Writing SQL With the Variant Data Type\n",
    "\n",
    "With Variant we not write SQL statements against any element of the original JSON file without knowing anything ahead of time about its original schema.  For traditional from_json methods to work, including inferring the JSON schema, each file should have the same consistent schema.  \n",
    "\n",
    "However with FHIR, every bundle recieved could have any number of resource arrays and in any order.  For example, there is no guarantee that the first reosource in the entry array is the Patient array.  Synthea, used here, and Redox (a Databricks partner) have very different standard practices for what it used or not used in the bundle, yet both are valid FHIR bundles.  \n",
    "\n",
    "For example, Redox always includes a bundle level meta object, and the first resource in their entry array is typically also a meta resource.  Synthea does not include the meta bundle object, and typically has the Patient resource first in the entry array for C-CDA converted bundles (though this need need be the case for all types of bundles).  Additonally Synthea occasionally produces malformed JSON files, whereas Redox only sends perfectly formed JSON files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bd9eefc-de6a-4c9f-99ac-8e683ac820a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below we show a few ways to write SQL against fully parsed JSON files in VARIANT.  \n",
    "\n",
    "- Using the `<variant_colname>:<path>::<datatype>` method where a single colon `:` indicates an entry path for the data selected, and the double colon `::` allows a shorthand for casting the value returned as the desired datatype.  \n",
    "- If we prefer to be versbose, we can always use the `CAST(expr as datatype)` function outright.  \n",
    "- And `variant_get` combines the familiar JSON paths that many are familiar with, including the `$.` notation, with a `CAST` where we provide the desired datatype as a string value. \n",
    "\n",
    "Note that if we don't cast the selected VARIANT data to a different datatype, then it will remain as a VARIANT column.  This is desirable to continue to use for structs until a final schema may be applied, however VARIANT column types can not be used as group by clauses for aggregation functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7296e0e5-344c-4bb5-b4fe-0eae74be66b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's Make a Map!"
    }
   },
   "outputs": [],
   "source": [
    "select \n",
    "  bundle_uuid\n",
    "  ,CAST(fhir:entry[0].resource.address[0].extension[0].extension[0].valueDecimal as double) as latitude\n",
    "  ,fhir:entry[0].resource.address[0].extension[0].extension[1].valueDecimal::double as longitude\n",
    "from \n",
    "  fhir_bronze_variant\n",
    "where \n",
    "  fhir is not null\n",
    "  and variant_get(fhir, '$.entry[0].resource.resourceType', 'STRING') = 'Patient';"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Streaming Bronze Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
